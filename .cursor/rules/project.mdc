---
description: 
globs: 
alwaysApply: true
---
On-Chain User Classification MVP: Technical Design

Overview

This document outlines the technical architecture and functional specification for a web-based MVP platform that analyzes EVM blockchain user data with AI. The system will connect to a selected EVM-compatible blockchain, collect on-chain data for user addresses, apply a machine learning model (trained on labeled data) to classify or profile users, and present the results through a web interface. The initial goal is to demonstrate AI-based user analysis (e.g. distinguishing bots vs. humans or profiling user types) without any targeting or recommendation features. Short iterative development is assumed, focusing on one blockchain first and ensuring a clear path for future expansion.

Functionality Breakdown

The MVP’s core functionalities can be broken down into the following components:
	•	Blockchain Data Ingestion: Connect to an EVM chain and retrieve relevant on-chain data for given user wallet addresses. This includes transaction history, token holdings, and contract interaction records.
	•	Data Processing & Storage: Parse and normalize the raw on-chain data into a structured format. Key metrics and features will be extracted (e.g. number of transactions, distinct counterparties, token balances) and stored for analysis.
	•	Labeled Dataset & Training: Assemble a labeled dataset of addresses with known classifications (e.g. “bot” vs “human” or specific user categories). Use this data to train an AI/ML model that can predict a user’s category based on on-chain behavior.
	•	AI Model Inference: Integrate the trained model into a backend service. Given a new wallet address (with its on-chain data), the model will output a classification or score indicating the likely user category.
	•	Web Interface (Frontend): Provide a user-facing web application where one can input a wallet address (or select from examples) and view the AI’s analysis. The UI will display the classification result, relevant supporting data (e.g. summary of the wallet’s activity), and allow simple interactions (like requesting analysis for another address).

Each of these functional components is detailed in the sections below, along with design decisions and implementation suggestions for the MVP.

EVM Chain Selection

Choosing the initial blockchain is crucial for accessing rich user data and ensuring the MVP’s feasibility. The selection criteria include network popularity, data availability, tooling support, and relevance of user behavior patterns. Below is a comparison of common EVM chains and justification for the choice:
	•	Ethereum Mainnet: The largest and most diverse ecosystem, with ~1.4 million transactions per day in early 2025 ￼. Ethereum hosts a wide variety of user activities (DeFi, NFTs, gaming, exchanges), which provides a rich dataset for classification. It also has excellent developer tooling and data sources (Infura/Alchemy RPC, Etherscan API, The Graph, etc.). Labeled data for Ethereum addresses is more accessible due to community research and explorer tags. The downside is the high data volume and complexity (and historically higher transaction costs, though read operations are off-chain). Despite the size, Ethereum is a strong candidate because it offers the most comprehensive user behavior patterns.
	•	BNB Chain (BSC): Very high throughput (~4 million transactions per day in 2024 ￼) and low fees, making it popular for many retail users and bots. BSC could be suitable for a bot-vs-human classification since it’s known for heavy bot activity in trading. It also has BSCScan (similar to Etherscan) and supported APIs. However, BSC’s user activity might be less diverse (predominantly trading and yield farming), and fewer public datasets of labeled addresses exist compared to Ethereum.
	•	Polygon (Polygon PoS): High activity with around 3+ million daily transactions in 2024 ￼, especially in gaming and NFT domains. Polygon offers low-cost interactions, attracting a broad user base including dApp users and potentially bots taking advantage of cheap fees. Developer support (PolygonScan, JSON-RPC, The Graph support) is solid. Polygon could be a good starting chain if the focus is on gaming or NFT user profiles. One consideration is that some advanced Ethereum tooling (certain datasets or labels) might not directly transfer to Polygon.

Justification: For the MVP, Ethereum is recommended as the initial chain due to its rich data and tooling. It provides the broadest variety of on-chain user behavior to train the model, and many established data collection methods exist. Ethereum’s extensive community resources (e.g. labeled address datasets on Kaggle ￼, public subgraphs, open APIs) will accelerate development. In summary, Ethereum maximizes the learning value for the model and offers robust infrastructure for data access. (If needed, the design will allow abstracting chain-specific details so that adding BSC, Polygon, etc. in the future is straightforward.)

On-Chain Data Collection

Collecting comprehensive on-chain data about a user’s address is the foundation of this system. The MVP will implement a data collection pipeline to gather the following for a given wallet address:
	•	Transaction history: All (or recent) transactions involving the address (both outgoing and incoming). This includes basic transfers and contract interactions.
	•	Token holdings and transfers: The set of ERC-20 tokens (and possibly ERC-721 NFTs) that the address holds, and historical token transfer events.
	•	Contract interaction details: Any interactions with smart contracts (DeFi protocols, NFT marketplaces, etc.), which can reveal user behavior patterns (e.g. using a DEX or a lending platform).

To achieve this, the data pipeline will leverage public blockchain data sources and APIs rather than running a full node from scratch (to keep the MVP lightweight):
	•	Public API Providers: Use reputable blockchain explorer APIs or data platforms. For Ethereum, the Etherscan API provides an easy way to fetch an address’s normal transactions, internal transactions, and token transfers (up to 10k records) ￼ ￼. These endpoints allow pagination through a wallet’s history. Another option is the Covalent API, which offers unified endpoints to fetch all transactions or token balances for an address in one call ￼ ￼. Such APIs simplify data gathering (e.g., Covalent’s /balances_v2/ returns all ERC-20 tokens an address holds with balances). Using these services with an API key can jump-start data collection with minimal setup.
	•	JSON-RPC via Provider Nodes: For more control, the system can connect to a blockchain node using JSON-RPC (through providers like Infura, Alchemy, QuickNode, or a self-hosted node). For example, Infura/Alchemy provide Ethereum RPC endpoints to retrieve data like transaction receipts, call data, and logs. The MVP can utilize web3 libraries (Web3.py for Python or Ethers.js for Node) to query details that explorer APIs might not directly provide. This approach is useful for on-demand queries (e.g., checking the code or events of a specific contract the user interacted with).
	•	The Graph / Subgraphs: When complex queries are needed (such as “find all interactions of this address with Uniswap contracts” or aggregating across many blocks), The Graph can be useful. The Graph’s existing subgraphs for popular protocols could be queried to see if the user address appears in those protocols. Alternatively, a custom subgraph can be written to index transactions or events relevant to user profiling (though this requires more setup and is likely beyond a quick MVP timeline ￼). For the MVP, The Graph might be optional, but it’s a scalable option for later (providing GraphQL queries with real-time indexed data).

Data Processing: Once raw data is fetched, the system will parse and normalize it:
	•	Transaction data will be cleaned into a unified format (fields like timestamp, sender, receiver, value, gas used, contract address if applicable, method signatures for contract calls, etc.).
	•	Token balance data will be structured as a list of assets (token name, symbol, balance, perhaps fiat value if needed for analysis).
	•	If raw logs or contract call data are collected, the pipeline can decode them using ABI information (e.g., using Ethers.js or web3.py to decode input data to find which function was called). For MVP, detailed decoding may not be necessary for classification, but recognizing standard token transfer events or known protocol interactions can enrich feature extraction.

Storage: For the MVP scale, a simple storage solution suffices:
	•	During development, data can be kept in memory or as JSON files for a few test addresses.
	•	As it evolves, use a lightweight database (such as PostgreSQL or MongoDB) to store processed transaction records and derived features for each address. A relational DB is convenient for joining data (address, transactions, tokens) and running queries for analysis. For example, after data collection, an address’s records could be stored in tables like transactions, token_holdings, etc., keyed by address.
	•	Storing data allows caching results – if the same address is analyzed multiple times, the system can avoid re-fetching from the blockchain every time. A cache expiration or update mechanism would be implemented to periodically refresh the data (especially token balances or new transactions).

By using the above methods, the MVP can quickly gather comprehensive on-chain information about any given user address without heavy infrastructure. This data serves as the input for feature engineering in the AI model pipeline.

Data Labeling Strategy

Training a supervised AI model requires a labeled dataset of user addresses with known categories (ground truth labels). The quality of these labels is critical for model accuracy ￼ ￼. The MVP will employ a combination of strategies to obtain labeled data for “user classification”:
	•	Open Datasets and Research: Leverage existing public datasets of labeled Ethereum addresses. For example, there are Kaggle datasets of Ethereum addresses classified by type (e.g. exchange, DeFi contract, hacker, normal user) ￼. One such dataset provides account types for 20,000 significant addresses ￼. These datasets often come from prior research or community efforts and can bootstrap the model training. Another source is academic research: for instance, a study categorized addresses into classes like “Phish/Hack,” “Scamming,” “Exchange,” and “ICO” wallets using transaction behavior ￼ ￼ – their published data or approach could inform our labeling.
	•	Heuristic Labeling: Create labeling rules for known behavior patterns. For distinguishing bots vs humans, one might label as “bot” any address that meets certain heuristics (e.g., extremely high frequency of transactions per minute, very short intervals between successive transactions, usage of automated contract patterns like flash loans or arbitrage contracts). Addresses that show normal human-like patterns (more sporadic activity, varied transaction sizes, interaction with user interfaces like NFT minting or ENS registration) could be labeled “human.” These rules can generate a preliminary labeled set, which can be refined manually.
	•	Known Entity Tags: Use blockchain explorer labels and community resources. Etherscan, for example, tags well-known addresses (like centralized exchange wallets, DeFi contract addresses, token sale wallets, etc.). While many user addresses are unlabeled, any address with an Etherscan tag can be leveraged for a specific category (e.g., an address labeled as “DexTrader” or identified as a bot by Etherscan comments). Other platforms like Chainalysis or Arkham Intelligence maintain databases of addresses with tags (e.g., identifying addresses belonging to the same entity or known bots), though access might be commercial. For MVP, even a small subset of clearly labeled addresses (like known bot operators or verified humans such as public ENS addresses of individuals) can serve as training examples.
	•	Manual Labeling for Prototype: Since the MVP is limited in scope, the team can manually label a modest number of addresses by inspecting their on-chain activity. For example, pick ~100 addresses and determine their type via manual analysis (looking at their transactions on a block explorer). The team’s domain knowledge can guide what constitutes a bot (maybe seeing interactions with known bot contracts or continuous activity 24/7) versus a normal user. If profiling user types (beyond bot/human), one could label categories like “NFT collector” (addresses that hold many NFTs and transact on OpenSea), “DeFi yield farmer” (addresses interacting with lending and DEX contracts frequently), “Exchange account” (addresses that primarily deposit/withdraw to exchanges), etc. This manual set, while small, can augment automated labels and serve to validate the model’s outputs.
	•	Ongoing Refinement: The labeling process can be iterative. Initially, use the above sources to train a model; then review misclassified cases to update the labels or add new training examples. In future, a human-in-the-loop approach could be used where the model flags uncertain classifications and a human expert labels them, gradually improving the training set (active learning). For example, the platform might incorporate user feedback or expert verification for borderline cases (though this is likely beyond the MVP phase, it’s good to design the system with the ability to update the model as new labels come in).

For the MVP, we assume a focused classification (such as Bot vs Human as a binary label for simplicity). We will gather a few hundred example addresses: some known bots, some likely human users, ensuring each class has sufficient examples. This labeled dataset will then be split into training and testing subsets for model development.

AI Model Pipeline

With data collected and labeled, the AI pipeline will transform the raw blockchain data into features suitable for machine learning, train a model, and integrate it for inference. The approach will be primarily supervised learning given we have labeled examples of user categories.

Feature Engineering & Preprocessing

Raw on-chain data needs to be distilled into a set of features that capture behavior patterns of a user. Designing good features is a key step since it directly affects model performance. For each wallet address (user), the system will compute a variety of features from their transaction and token data, for example:
	•	Activity Metrics: Total number of transactions made, frequency of transactions (e.g. average transactions per day), time interval between transactions (min/avg/max). Very high-frequency or regular interval activity might indicate bot automation ￼.
	•	Transaction Characteristics: Average and median transaction value (in ETH), distribution of transaction sizes, gas fees spent (total and average). Bots might spend gas differently (e.g. constantly bidding high gas for priority) compared to casual users.
	•	Counterparties: Number of unique addresses interacted with. A human user might interact with a diverse set of addresses (friends, various contracts), whereas a specialized bot might interact only with specific contract addresses (like one DeFi pool or a target contract).
	•	Smart Contract Interactions: Count of distinct contracts called. We can include binary flags for interacting with certain known protocols: e.g., used Uniswap (yes/no), used OpenSea, used a Token Sale contract, etc. This can be derived by checking if any transaction to the address matches known contract addresses of those platforms. For user profiling, these are especially useful (e.g. if an address has interacted with multiple DEXes and yield farms, it’s likely a DeFi trader).
	•	Token Holdings: Number of different token types held; presence of any notable tokens or NFTs. For instance, holding many NFT assets could be a strong indicator of an NFT collector type. Conversely, a bot might hold mostly just ETH (as fuel) and maybe one token it’s arbitraging.
	•	Account Age and Lifespan: How old the address is (time since first transaction) and how active it’s been recently. A very new address that suddenly does many transactions could be an airdrop farmer or bot. Time between first and last transaction is a feature used in illicit account detection ￼.
	•	Transaction Patterns: e.g., ratio of incoming vs outgoing transactions, average time of day of activity (bots might run 24/7, humans might cluster during waking hours), and any cyclic patterns. We can compute if the address tends to transact at specific times or continuously.
	•	Graph-based Features: (Optional for MVP) Simple graph metrics like PageRank or centrality of the address in the interaction network (using number of connections). This can highlight if an address is a hub (like an exchange) or an isolated node. For MVP, we might not go deep into graph algorithms, but we will note if an address is directly connected to any known entities (e.g., has transacted with a known exchange or a known scam address – which might indicate the user category).

All features will be numerical or categorical values in a single row per address. Before training, features are normalized or scaled as needed (e.g., log-scaling highly skewed features like transaction counts). Categorical features (like “used Uniswap yes/no”) can be one-hot encoded. We will also be careful about data leakage – features like “last activity time” must be relative if used, to avoid using future information when predicting on new addresses. The final result is a feature vector for each labeled address which the model will train on.

Model Architecture & Training

For the initial model, a classic supervised classification algorithm will be used, given the structured nature of our features. The choice of model balances simplicity, interpretability, and accuracy:
	•	Machine Learning Model: A tree-based model such as Random Forest or XGBoost (Extreme Gradient Boosting) is a strong starting point. Prior research on Ethereum address classification found XGBoost to perform well, achieving around 75% accuracy in multi-class address classification ￼. These models handle mixed data types well and provide feature importance metrics (useful for understanding which on-chain features matter most in distinguishing bots vs humans, etc.). We can begin with XGBoost for its proven effectiveness in similar problems and its ability to handle non-linear feature interactions.
	•	Alternative Models: As data grows, we may try more complex models. For example, a Graph Neural Network (GNN) could directly operate on the transaction graph of addresses (treating addresses as nodes and transactions as edges) to classify users by analyzing network structure. GNNs are promising for on-chain analysis as they capture relational information, but they require more data and infrastructure (and are harder to explain) – likely beyond the MVP. Another approach is a Neural Network on the feature vector (a multi-layer perceptron), but given the tabular nature of features, tree-based methods often perform as well or better on smaller datasets. We will note these as future enhancements, but they won’t be our first implementation. (If time permits, we might compare a simple neural network to XGBoost to see which gives better validation accuracy).
	•	Training Process: The labeled dataset will be split into training and test sets (for example, 80% of addresses for training, 20% held-out for evaluation). Since our dataset might be relatively small (hundreds or a few thousand addresses), we will use cross-validation during training to more robustly estimate performance. The training loop involves feeding the features and labels into the model, tuning hyperparameters as needed (e.g., number of trees and depth for Random Forest/XGBoost). Given the MVP scope, we might do a manual or grid search for hyperparameters to optimize accuracy. Training can be done offline (e.g., as a one-time Python script using scikit-learn or XGBoost library). The output will be a trained model (model file) that can be saved for use in the web application.
	•	Label Categories: If we do binary classification (Bot vs Human), the model will output a probability or confidence for “Bot” class which we threshold (or directly output the class with highest probability). If we do multi-class profiling (say, categories: {Bot, Human-DeFi, Human-NFT, Exchange, etc.}), the model will output a probability distribution over these classes. The training in that case would use a suitable loss (e.g., softmax with cross-entropy for multi-class) and we’d ensure each class has enough examples or use techniques like class weighting if the dataset is imbalanced.

Model Evaluation

During development, we will evaluate the model to ensure it generalizes and meets the MVP’s objectives:
	•	Accuracy and F1 Score: On the test set, we’ll compute accuracy for overall performance. However, with class-imbalanced data (likely far fewer bots than humans, or vice versa, depending on sources), accuracy can be misleading. So we will check precision and recall for each class, and the F1-score (the harmonic mean of precision and recall) which was used in related research ￼. For example, if classifying bots, we care that we catch most bots (high recall) while not mislabeling too many humans as bots (precision).
	•	Confusion Matrix: Examine which classes or labels get confused. If the model often mistakes a certain type of human as a bot, we might need more features or better labeling for that scenario. The confusion matrix gives insight into where the model struggles.
	•	Feature Importance: For tree-based models, we can extract the top features that influence the predictions. This not only validates our intuition (e.g., we might expect “transactions per day” to be a top feature for bots) but also provides transparency. If some features are unexpectedly important or if any are useless, we can refine the feature set.
	•	Iterate if Needed: If initial performance is not satisfactory (say the model only achieves 60% accuracy in distinguishing bots vs humans), we’ll revisit the data: possibly the labels are noisy or inconsistent, or important behavior indicators are missing from features. We can incorporate additional features (e.g., analyzing transaction timing more granularly or using external data like time zones) or gather more training examples. Since this is an MVP, we aim for a reasonable accuracy to demonstrate the concept (for instance, >80% accuracy in the binary classification task would be a good outcome to showcase).

The trained and evaluated model will then be packaged for integration. Likely, we will export the model (e.g., as a serialized .pkl file for scikit-learn/XGBoost or a saved booster model) so that it can be loaded by the backend server for inference.

System Architecture

The overall system architecture ties together the data pipeline, the AI model, and the web interface into a cohesive MVP product. Below is a high-level representation of the architecture and data flow:

High-level architecture of the MVP platform. Data is collected from the blockchain into a storage/processing layer, an ML model is trained on labeled data, and the trained model is deployed on the backend to serve the frontend queries.

As illustrated above, the architecture consists of distinct components for data ingestion, model training, and the web application (backend + frontend), each with a defined role:
	•	Blockchain Node / Data Source: The EVM chain itself (Ethereum in this case) and associated APIs (Etherscan, etc.) act as the source of truth for on-chain data. The system will interact with these sources to fetch required data. This interaction is read-only via RPC calls or REST API calls.
	•	Data Collector Module: This is a backend process or service that handles fetching raw data for a given address. In the MVP, this might be a simple library module called on-demand. For example, when the user inputs an address on the frontend, the backend triggers the Data Collector to gather that address’s transactions from Etherscan API and token balances from Covalent API. The Data Collector encapsulates all the logic of calling external APIs, handling API keys and rate limits, and merging data from multiple sources.
	•	Data Storage & Processing: Once data is fetched, it goes through processing (feature extraction as described earlier). In an MVP, heavy storage might not be required, but we design a logical storage layer. This could be an in-memory store or a database where we keep the address’s data and computed features. In a production system, one might have a data warehouse or pipeline (the slides mention SQL-based storage and indexing for real-time analysis in advanced systems). For the MVP, the storage can simply be a Python data structure or a small database. This layer ensures that if multiple requests ask for the same address analysis, we don’t recompute everything from scratch each time. It also decouples data fetching from model inference (we could fetch data first, ensure it’s complete, then call the model).
	•	Model Training Environment: The training of the AI model is done offline (not part of the real-time user request flow). This could be a Jupyter notebook or script using the collected and labeled data. It produces the Trained Model artifact (highlighted in the diagram) which is then saved. The model training environment will likely share access to the stored data or have its own copy of the dataset prepared for training.
	•	Model Inference Service (Backend API): This is the backend server that the web frontend interacts with. It loads the trained model artifact into memory. When a request comes in to analyze a particular address, the backend will ensure the data for that address is collected (using the Data Collector), then perform feature extraction (if not already done), then feed those features into the loaded model to get a prediction. The backend then returns the result (classification category, confidence score, and possibly the underlying features or explanation) to the frontend. This service can be a simple REST API. For instance, a GET /classify?address=0xABC... endpoint which triggers the process and responds with JSON data like {"address": "...", "label": "Bot", "confidence": 0.92, "features": {...}}. The backend orchestrates between the data layer and the model.
	•	Frontend Web UI: The user interface is a web application (could be a single-page app or a simple multi-page app) that allows the user to input an address or select one, and then displays the analysis. The frontend sends requests to the backend API and awaits the result. Once the result is received, the UI will render the classification outcome and any supplementary info (for example, a summary of the address’s activity or key reasons for the classification if available). The frontend component is separate from the backend, communicating over HTTP/HTTPS.

Workflow Summary: When a user uses the app, the flow is:
	1.	User enters an address on the frontend and clicks “Analyze”.
	2.	The frontend (browser) makes an API call to the backend (with the address).
	3.	The backend API receives the request. It checks if data for this address is already in the database/cache. If not, it invokes the Data Collector to fetch transactions and token info from the blockchain sources.
	4.	The raw data is processed into features. These features are fed into the ML model (loaded in memory).
	5.	The model outputs a prediction (e.g., “Bot” with 92% confidence). The backend formats this result (and possibly some key feature values or explanations) into a response.
	6.	The frontend receives the response and updates the UI to show the classification result to the user, including any relevant details (e.g., “Label: Bot (92% confidence)” and maybe a breakdown like “This address made 5000 transactions in the last month and interacted with only 1 contract, which is unusual for a human user.”).
	7.	The user can then input another address or navigate through the app as needed.

Because the MVP is focused on demonstration, we expect relatively low request volume (e.g., a few addresses analyzed during a demo). Therefore, this architecture can be kept simple without heavy scaling considerations. If scaling or real-time needs increase, the architecture can be evolved by adding message queues (for asynchronous data fetching), more robust caching, or even streaming data pipelines (not needed at MVP stage).

Implementation Suggestions

Backend Technology Stack

For the backend including data handling and model inference, the stack should facilitate rapid development, integration with blockchain APIs, and machine learning libraries:
	•	Language: Python is a strong choice for the backend in this project. Python has rich libraries for blockchain interaction (e.g., web3.py for JSON-RPC), HTTP APIs (Flask/FastAPI for building the REST endpoints), and machine learning (scikit-learn, XGBoost, PyTorch, etc.). The team can easily integrate the model training code and the serving code in Python. For example, the model could be trained in a Jupyter notebook and then the same model loaded in a Flask app. Python’s ecosystem will speed up development of the data processing (pandas for data manipulation, etc.) and ML inference.
	•	Alternative: If the team has more expertise in Node.js/TypeScript on the blockchain side, it’s possible to split responsibilities: use Node.js for data collection (with Ethers.js and calling external APIs) and a Python microservice for ML. However, for an MVP, simplicity is key – running everything in one process or at least one language might reduce complexity. FastAPI (Python) could handle both fetching from APIs (it can call external REST endpoints for Etherscan/Covalent) and running the model.
	•	Data Libraries: Utilize requests or an official SDK to call APIs (Etherscan has community Python SDKs; Covalent has a Python client as well). Use web3.py for any direct RPC calls needed (like getting transaction receipt to decode events if needed). For data parsing, Python’s standard libraries and regex can help parse through transaction input data if required.
	•	Database: For MVP, using an in-memory store or lightweight file-based DB is fine. For example, SQLite can be used via SQLAlchemy in Python to store fetched data. This avoids setting up a separate DB server. If a more robust solution is needed, PostgreSQL is a good option (and the SQL can help in analytical queries). The schema might include tables: address_summary, transactions, token_holdings, etc. But given time constraints, the team might even skip a formal DB and use Python dictionaries or Pandas DataFrames to hold data during a session.
	•	Model Serving: Once the model is trained (say using scikit-learn or XGBoost), the model object or parameters can be saved (e.g., pickle the model). The backend service at startup will load this model into memory. Every time a request comes, it simply calls model.predict(features) to get the output. This is very fast (milliseconds) compared to data fetching time. If using a framework like TensorFlow or PyTorch (in case a neural network was used), one could export the model or just keep the Python code that constructs the model and weights. The scale is small, so there is no need for a specialized model server (like TensorFlow Serving); a basic function call is sufficient.
	•	Execution Environment: Running the backend on a cloud VM or even a serverless function is possible. A small Flask app could be deployed on AWS EC2, Heroku, or similar. Given this is an MVP for demonstration, even running locally on a laptop is fine. If deploying, ensure API keys (for Etherscan etc.) are securely stored (as environment variables).

Frontend Technology Stack

The frontend should be lightweight and intuitive, showcasing the results clearly:
	•	Framework: React.js is a popular choice for building dynamic single-page applications and would work well if the team wants a modern UI. With React, one can create a simple form for address input and state management for showing results. However, if the team is less front-end heavy, even a basic HTML/JavaScript page or a minimal framework (like Vue or just jQuery) could suffice for MVP. The key is to be able to call the backend API and update the page with results.
	•	UI Design: The interface will likely consist of:
	•	An input field for the user to enter a wallet address (with basic validation for correct hex format).
	•	A submit button to trigger analysis.
	•	An area to display results: this could be a card or section that appears with the classification outcome. For example, a colored label “Human User” or “Bot” and possibly a percentage confidence. Additionally, a small table or list of key metrics can be shown (for transparency). For instance: “Total Transactions: 120”, “Active for: 2 years”, “Distinct Tokens Held: 5”, etc., to give the user context on why the classification is made.
	•	If doing multi-class profiling, the UI might display something like a profile tag (e.g., “DeFi Trader”) along with a short description (“This wallet frequently uses DeFi protocols and trades assets, indicating a DeFi-oriented user.”).
	•	Interactivity: When the user hits “Analyze”, the app should show a loading indicator while waiting for the backend (since data fetch might take a couple of seconds if retrieving from APIs). Once the response comes, update the result section. You may also allow the user to try a set of example addresses via clickable buttons (for demonstration, you might preload some interesting addresses like a known bot and a known normal user to show contrast).
	•	Embedding Charts: If time permits, visualizing some data can enhance the presentation. For example, a small chart of the user’s transaction frequency over time, or a pie chart of their token holdings. Libraries like Chart.js or D3.js could be integrated for this. However, these are optional for MVP; the primary goal is to present the classification clearly.
	•	Styling: Use a simple CSS framework (Bootstrap or Material UI) to make the interface clean and responsive. This will save time on design. A clean layout might have the input at top and results below. Ensure mobile-responsiveness if needed (just in case someone opens it on a mobile device during demonstration).

The frontend will communicate with the backend via AJAX/fetch calls. If using React, the built-in fetch() API or Axios can be used to call the backend endpoint and handle the JSON response.

Web App Interaction & UX

From a user’s perspective, the web application will function as follows:
	1.	The user navigates to the MVP web app. They are presented with a brief introduction or title (e.g., “On-Chain User Analyzer”) and an input form for a wallet address.
	2.	The user enters a valid address (or selects a provided example) and submits. The app might immediately show something like “Analyzing on-chain activity for this address…” to inform the user that the request is processing.
	3.	The backend processes the request as described earlier. While waiting, the UI could show a spinner icon.
	4.	When the result returns, the UI transitions to show the output:
	•	A clear classification label: e.g., “Result: Likely Human User” or “Result: Bot Account suspected”. If a probability or confidence is available, it can be shown (perhaps as a subtitle or tooltip, e.g., “Confidence: 92%”).
	•	Supporting details: The app can display key features about the address that justify the classification. For example:
	•	“Active for 3 months, 300 transactions, interacts with 25 other addresses.”
	•	“Uses 10 different tokens; notable protocol interactions: Uniswap, Compound.”
	•	“Transaction rate: average 10 per day, with bursts of 50+ per hour (pattern suggests automated activity).”
	•	These details provide transparency and make the tool educational. It helps the user trust the AI judgment by seeing data points.
	•	If doing bot/human classification, perhaps a short explanatory line: “This address exhibits patterns consistent with a bot (very high frequency, repeating transaction sizes).” Or if human: “This address shows varied activity over longer periods, which is typical of a human user.”
	5.	The user can then input another address. The interface should allow iterative use without page reload (if using a single-page app). Each time, update the result section accordingly. We might also maintain a history of queries in that session (like a list of addresses analyzed and their result) for convenience.
	6.	If an error occurs (e.g., address not found or invalid input), the UI should display a friendly error message (“Invalid address format” or “No data found for this address”).

Security and privacy considerations are minimal here since we are analyzing public data and not storing sensitive user info. However, we ensure to not expose our API keys on the frontend (all API calls to Etherscan/Covalent happen in the backend). The web app is essentially a read-only analytics dashboard for blockchain data.

Implementation Timeline and Extensions

In implementing this MVP, the team could take an iterative approach:
	•	First, get the data collection working for a couple of addresses (e.g., via a simple script that fetches data and prints features).
	•	Second, label a small dataset and train a basic model offline.
	•	Third, integrate the model into a simple backend API and test it with known examples.
	•	Finally, build the frontend to interact with the backend and polish the user experience.

While the MVP focuses on one chain and limited functionality, the design is made modular so that future enhancements are possible. For instance, adding support for another EVM chain would involve writing a new data collector adapter for that chain’s APIs and retraining/adjusting the model if needed for different chain characteristics. Similarly, the classification logic could be extended to more categories or even unsupervised clustering for discovering user segments once enough data is gathered (the team’s long-term vision includes advanced techniques like Graph Neural Networks and even quantum-enhanced models as noted in internal research, but those are beyond the MVP scope).

In conclusion, this technical design provides a roadmap to build a minimal yet end-to-end functional prototype for on-chain user classification. It balances simplicity (leveraging existing tools and straightforward ML models) with a clear structure that mirrors more scalable architectures. The end result will be a web app that demonstrates how AI can derive insights from transparent blockchain data, identifying whether a wallet is likely a bot or a human (or other profile) and presenting that insight in an accessible manner. The MVP can serve as a proving ground for the concept and a foundation for more sophisticated user analysis features in the future.